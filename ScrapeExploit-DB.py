#!/usr/bin/python3

import json
from selenium import webdriver
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import StaleElementReferenceException
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from time import sleep


def main():
    opts = Options()
    opts.add_argument('Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)')
    browser = webdriver.Chrome(options=opts, executable_path='/Users/Daman2177/Downloads/chromedriver')
    browser.get('https://www.exploit-db.com')
    sleep(5)
    browser.find_element_by_xpath("//select[@name='exploits-table_length']/option[text()='120']").click()
    sleep(5)
    all_exploit_data = scrape_data(browser)
    browser.close()
    output_to_json(all_exploit_data)


def scrape_data(browser):
    all_exploit_data = []
    data_scraped = 0
    ignored_exceptions = (NoSuchElementException, StaleElementReferenceException)
    wait = WebDriverWait(browser, 200, ignored_exceptions=ignored_exceptions)
    xpath_table_rows = '(//table[@id = "exploits-table"]/tbody/tr[@role = "row"])'

    while True:
        try:
            table = wait.until(EC.presence_of_all_elements_located(
                (By.XPATH, xpath_table_rows)))

            for row_num in range(1, len(table)):
                row = f'[{row_num}]'
                exploit_data = {}
                wait.until(EC.presence_of_element_located((By.XPATH, xpath_table_rows + row)))

                date = wait.until(EC.presence_of_element_located((By.XPATH, xpath_table_rows + row + '/td[1]')))
                verified = wait.until(EC.presence_of_element_located((By.XPATH, xpath_table_rows + row + '/td[4]/i')))
                id = wait.until(EC.presence_of_element_located((By.XPATH, xpath_table_rows + row + '/td[5]/a')))
                type = wait.until(EC.presence_of_element_located((By.XPATH, xpath_table_rows + row + '/td[6]/a')))
                platform = wait.until(EC.presence_of_element_located((By.XPATH, xpath_table_rows + row + '/td[7]/a')))
                author = wait.until(EC.presence_of_element_located((By.XPATH, xpath_table_rows + row + '/td[8]/a')))

                exploit_data['Date'] = date.text
                exploit_data['ID'] = str(id.get_attribute('href'))[-5::]
                exploit_data['Title'] = id.text
                exploit_data['Author'] = author.text
                exploit_data['Type'] = type.text
                exploit_data['Platform'] = platform.text

                if str(verified.get_attribute('class')) == 'mdi mdi-check mdi-18px':
                    exploit_data['Verified'] = True
                else:
                    exploit_data['Verified'] = False

                all_exploit_data.append(exploit_data)
                data_scraped += 1
                print(exploit_data)

            browser.find_element_by_xpath('//*[@id="exploits-table_next"]/a').click()
            sleep(7)

        except NoSuchElementException:
            break

        except StaleElementReferenceException:
            break

    print(f'Data scraped: {data_scraped}')
    return all_exploit_data


def output_to_json(all_exploit_data):
    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(all_exploit_data, f, ensure_ascii=False, indent=4)


if __name__ == "__main__":
    main()